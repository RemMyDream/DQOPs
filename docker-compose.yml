services:
  # PostgreSQL - Source Database
  postgres:
    image: postgres:15
    container_name: postgres-source
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: sourcedb
    command:
      - "postgres"
      - "-c"
      - "wal_level=logical"
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - data-pipeline

  # MinIO - Object Storage
  minio:
    image: minio/minio:latest
    container_name: minio
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: password
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    networks:
      - data-pipeline

  # MinIO Client - Initialize buckets
  minio-init:
    image: minio/mc:latest
    container_name: minio-init
    depends_on:
      - minio
    entrypoint: >
      /bin/sh -c "
      sleep 5;
      mc alias set myminio http://minio:9000 admin password;
      mc mb myminio/bronze --ignore-existing;
      mc mb myminio/silver --ignore-existing;
      mc mb myminio/gold --ignore-existing;
      mc mb myminio/warehouse --ignore-existing;
      mc mb myminio/kubeflow --ignore-existing;
      echo 'Buckets created successfully';
      "
    networks:
      - data-pipeline

  # Spark Master
  spark-master:
    build:
      context: .
      dockerfile: Dockerfile.spark
    image: custom-spark:3.5.3
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
    ports:
      - "8080:8080"  # Master Web UI
      - "7077:7077"  # Master Port
      - "4040:4040"  # Application Web UI
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    networks:
      - data-pipeline
    volumes:
      - ./spark/jobs:/opt/spark/jobs
      - ./spark/jars:/opt/spark/jars/extra
      - ./notebooks:/app/notebooks

  # Spark Worker
  spark-worker-1:
    image: custom-spark:3.5.3
    container_name: spark-worker-1
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_WEBUI_PORT=8081
    ports:
      - "8081:8081"  # Worker-1 Web UI (host:container)
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    networks:
      - data-pipeline
    volumes:
      - ./spark/jobs:/opt/spark/jobs
      - ./spark/jars:/opt/spark/jars/extra
      - ./notebooks:/app/notebooks

  spark-worker-2:
    image: custom-spark:3.5.3
    container_name: spark-worker-2
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_WEBUI_PORT=8081
    ports:
      - "8082:8081"  # Worker-2 Web UI (map host 8082 -> container 8081)
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    networks:
      - data-pipeline
    volumes:
      - ./spark/jobs:/opt/spark/jobs
      - ./spark/jars:/opt/spark/jars/extra
      - ./notebooks:/app/notebooks

  # PostgreSQL - OpenMetadata Metadata Store
  postgres-metadata:
    image: docker.getcollate.io/openmetadata/postgresql:1.11.0
    container_name: postgres-metadata
    restart: always
    command: "--work_mem=10MB"
    environment:
      POSTGRES_USER: openmetadata_user
      POSTGRES_PASSWORD: openmetadata_password
      #POSTGRES_DB: openmetadata_db
    ports:
      - "5433:5432"
    healthcheck:
      test: ["CMD-SHELL", "psql -U openmetadata_user -tAc 'select 1' -d openmetadata_db"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 60s
    volumes:
      - postgres_metadata_data:/var/lib/postgresql/data
    networks:
      - data-pipeline

  # Elasticsearch - OpenMetadata Search Engine
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.4
    container_name: openmetadata-elasticsearch
    environment:
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms1024m -Xmx1024m
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
      - "9300:9300"
    healthcheck:
      test: "curl -s http://localhost:9200/_cluster/health?pretty | grep status | grep -qE 'green|yellow' || exit 1"
      interval: 15s
      timeout: 10s
      retries: 10
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    networks:
      - data-pipeline
  
  # OpenMetadata Migration Service
  execute-migrate-all:
    container_name: execute_migrate_all
    image: docker.getcollate.io/openmetadata/server:1.11.0
    command: "./bootstrap/openmetadata-ops.sh migrate"
    environment:
      # Database Configuration
      DB_DRIVER_CLASS: org.postgresql.Driver
      DB_SCHEME: postgresql
      DB_USER: openmetadata_user
      DB_USER_PASSWORD: openmetadata_password
      DB_HOST: postgres-metadata
      DB_PORT: 5432
      OM_DATABASE: openmetadata_db
      
      # Migration Configuration
      MIGRATION_LIMIT_PARAM: 1200
      
      # Elasticsearch Configuration
      SEARCH_TYPE: elasticsearch
      ELASTICSEARCH_HOST: elasticsearch
      ELASTICSEARCH_PORT: 9200
      ELASTICSEARCH_SCHEME: http
      
      # Authentication Configuration
      AUTHENTICATION_PROVIDER: basic
      AUTHORIZER_CLASS_NAME: org.openmetadata.service.security.DefaultAuthorizer
      AUTHORIZER_REQUEST_FILTER: org.openmetadata.service.security.JwtFilter
      AUTHORIZER_ADMIN_PRINCIPALS: '[admin]'
      AUTHORIZER_PRINCIPAL_DOMAIN: open-metadata.org
    depends_on:
      postgres-metadata:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    networks:
      - data-pipeline
  
  # OpenMetadata Server
  openmetadata-server:
    container_name: openmetadata_server
    restart: always
    image: docker.getcollate.io/openmetadata/server:1.11.0
    environment:
      # Server Configuration
      OPENMETADATA_CLUSTER_NAME: ${OPENMETADATA_CLUSTER_NAME:-openmetadata}
      SERVER_PORT: ${SERVER_PORT:-8585}
      SERVER_ADMIN_PORT: ${SERVER_ADMIN_PORT:-8586}
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      
      # Migration Configuration
      MIGRATION_LIMIT_PARAM: 1200
      DATABASE_MIGRATION_VALIDATE_ONLY: "false"
      
      # Database Configuration (PostgreSQL)
      DB_DRIVER_CLASS: org.postgresql.Driver
      DB_SCHEME: postgresql
      DB_USER: openmetadata_user
      DB_USER_PASSWORD: openmetadata_password
      DB_HOST: postgres-metadata
      DB_PORT: 5432
      OM_DATABASE: openmetadata_db
      DB_PARAMS: allowPublicKeyRetrieval=true&useSSL=false&serverTimezone=UTC
      
      # Elasticsearch Configuration
      SEARCH_TYPE: elasticsearch
      ELASTICSEARCH_HOST: elasticsearch
      ELASTICSEARCH_PORT: 9200
      ELASTICSEARCH_SCHEME: http
      ELASTICSEARCH_USER: ${ELASTICSEARCH_USER:-""}
      ELASTICSEARCH_PASSWORD: ${ELASTICSEARCH_PASSWORD:-""}
      ELASTICSEARCH_CONNECTION_TIMEOUT_SECS: ${ELASTICSEARCH_CONNECTION_TIMEOUT_SECS:-5}
      ELASTICSEARCH_SOCKET_TIMEOUT_SECS: ${ELASTICSEARCH_SOCKET_TIMEOUT_SECS:-60}
      ELASTICSEARCH_BATCH_SIZE: ${ELASTICSEARCH_BATCH_SIZE:-100}
      ELASTICSEARCH_INDEX_MAPPING_LANG: ${ELASTICSEARCH_INDEX_MAPPING_LANG:-EN}
      
      # Authentication & Authorization
      AUTHORIZER_CLASS_NAME: ${AUTHORIZER_CLASS_NAME:-org.openmetadata.service.security.DefaultAuthorizer}
      AUTHORIZER_REQUEST_FILTER: ${AUTHORIZER_REQUEST_FILTER:-org.openmetadata.service.security.JwtFilter}
      AUTHORIZER_ADMIN_PRINCIPALS: ${AUTHORIZER_ADMIN_PRINCIPALS:-[admin]}
      AUTHORIZER_PRINCIPAL_DOMAIN: ${AUTHORIZER_PRINCIPAL_DOMAIN:-open-metadata.org}
      AUTHORIZER_INGESTION_PRINCIPALS: ${AUTHORIZER_INGESTION_PRINCIPALS:-[ingestion-bot]}
      AUTHENTICATION_PROVIDER: ${AUTHENTICATION_PROVIDER:-basic}
      AUTHENTICATION_PUBLIC_KEYS: ${AUTHENTICATION_PUBLIC_KEYS:-[http://localhost:8585/api/v1/system/config/jwks]}
      AUTHENTICATION_AUTHORITY: ${AUTHENTICATION_AUTHORITY:-http://localhost:8585}
      AUTHENTICATION_CALLBACK_URL: ${AUTHENTICATION_CALLBACK_URL:-http://localhost:8585/callback}
      AUTHENTICATION_JWT_PRINCIPAL_CLAIMS: ${AUTHENTICATION_JWT_PRINCIPAL_CLAIMS:-[email,preferred_username,sub]}
      AUTHENTICATION_ENABLE_SELF_SIGNUP: ${AUTHENTICATION_ENABLE_SELF_SIGNUP:-true}
      
      # JWT Configuration
      RSA_PUBLIC_KEY_FILE_PATH: ${RSA_PUBLIC_KEY_FILE_PATH:-./conf/public_key.der}
      RSA_PRIVATE_KEY_FILE_PATH: ${RSA_PRIVATE_KEY_FILE_PATH:-./conf/private_key.der}
      JWT_ISSUER: ${JWT_ISSUER:-open-metadata.org}
      JWT_KEY_ID: ${JWT_KEY_ID:-Gb389a-9f76-gdjs-a92j-0242bk94356}
      
      # Pipeline Service (Airflow Integration)
      PIPELINE_SERVICE_CLIENT_ENABLED: ${PIPELINE_SERVICE_CLIENT_ENABLED:-true}
      PIPELINE_SERVICE_CLIENT_ENDPOINT: ${PIPELINE_SERVICE_CLIENT_ENDPOINT:-http://ingestion:8080}
      PIPELINE_SERVICE_CLIENT_HEALTH_CHECK_INTERVAL: ${PIPELINE_SERVICE_CLIENT_HEALTH_CHECK_INTERVAL:-300}
      SERVER_HOST_API_URL: ${SERVER_HOST_API_URL:-http://openmetadata-server:8585/api}
      PIPELINE_SERVICE_CLIENT_VERIFY_SSL: ${PIPELINE_SERVICE_CLIENT_VERIFY_SSL:-no-ssl}
      PIPELINE_SERVICE_CLIENT_CLASS_NAME: ${PIPELINE_SERVICE_CLIENT_CLASS_NAME:-org.openmetadata.service.clients.pipeline.airflow.AirflowRESTClient}
      PIPELINE_SERVICE_CLIENT_SECRETS_MANAGER_LOADER: ${PIPELINE_SERVICE_CLIENT_SECRETS_MANAGER_LOADER:-noop}
      
      # Airflow Parameters
      AIRFLOW_USERNAME: ${AIRFLOW_USERNAME:-admin}
      AIRFLOW_PASSWORD: ${AIRFLOW_PASSWORD:-admin}
      AIRFLOW_TIMEOUT: ${AIRFLOW_TIMEOUT:-10}
      FERNET_KEY: ${FERNET_KEY:-jJ/9sz0g0OHxsfxOoSfdFdmk3ysNmPRnH3TUAbz3IHA=}
      
      # Secrets Manager
      SECRET_MANAGER: ${SECRET_MANAGER:-db}
      
      # Event Monitoring
      EVENT_MONITOR: ${EVENT_MONITOR:-prometheus}
      EVENT_MONITOR_BATCH_SIZE: ${EVENT_MONITOR_BATCH_SIZE:-10}
      
      # Email Configuration (disabled by default)
      AUTHORIZER_ENABLE_SMTP: ${AUTHORIZER_ENABLE_SMTP:-false}
      OM_EMAIL_ENTITY: ${OM_EMAIL_ENTITY:-OpenMetadata}
      OM_SUPPORT_URL: ${OM_SUPPORT_URL:-https://slack.open-metadata.org}
      
      # Memory Configuration
      OPENMETADATA_HEAP_OPTS: ${OPENMETADATA_HEAP_OPTS:--Xmx1G -Xms1G}
      
      # Security
      MASK_PASSWORDS_API: ${MASK_PASSWORDS_API:-false}
      
      # Web Configuration
      WEB_CONF_URI_PATH: ${WEB_CONF_URI_PATH:-/api}
    depends_on:
      postgres-metadata:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
      execute-migrate-all:
        condition: service_completed_successfully
    ports:
      - "8585:8585"
      - "8586:8586"
    networks:
      - data-pipeline
    healthcheck:
      test: [ "CMD", "wget", "-q", "--spider",  "http://localhost:8586/healthcheck" ]
      interval: 30s
      timeout: 10s
      retries: 5

  # Setup Airflow User - runs after postgres-metadata is healthy
  setup-airflow-user:
    container_name: setup_airflow_user
    image: postgres:15
    depends_on:
      postgres-metadata:
        condition: service_healthy
    environment:
      PGPASSWORD: openmetadata_password
    volumes:
      - ./scripts/setup-airflow-user.sh:/setup-airflow-user.sh
    command: ["/bin/bash", "/setup-airflow-user.sh"]
    networks:
      - data-pipeline

  # OpenMetadata Ingestion Service (Airflow)
  ingestion:
    container_name: openmetadata_ingestion
    image: docker.getcollate.io/openmetadata/ingestion:1.11.0
    depends_on:
      elasticsearch:
        condition: service_started
      postgres-metadata:
        condition: service_healthy
      openmetadata-server:
        condition: service_started
      setup-airflow-user:
        condition: service_completed_successfully
    environment:
      # Airflow API Configuration
      AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session"
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__OPENMETADATA_AIRFLOW_APIS__DAG_GENERATED_CONFIGS: "/opt/airflow/dag_generated_configs"

      # Database Configuration for Airflow
      DB_HOST: ${AIRFLOW_DB_HOST:-postgres-metadata}
      DB_PORT: ${AIRFLOW_DB_PORT:-5432}
      AIRFLOW_DB: ${AIRFLOW_DB:-airflow_db}
      DB_USER: ${AIRFLOW_DB_USER:-airflow_user}
      DB_SCHEME: ${DB_SCHEME:-postgresql+psycopg2}
      DB_PASSWORD: ${AIRFLOW_DB_PASSWORD:-airflow_pass}
      DB_PROPERTIES: ${AIRFLOW_DB_PROPERTIES:-}
      
      # OpenMetadata Server Configuration
      OPENMETADATA_SERVER_URL: ${OPENMETADATA_SERVER_URL:-http://openmetadata-server:8585/api}
      
      # Optional: Lineage Backend Configuration
      # AIRFLOW__LINEAGE__BACKEND: airflow_provider_openmetadata.lineage.backend.OpenMetadataLineageBackend
      # AIRFLOW__LINEAGE__AIRFLOW_SERVICE_NAME: local_airflow
      # AIRFLOW__LINEAGE__OPENMETADATA_API_ENDPOINT: http://openmetadata-server:8585/api
      # AIRFLOW__LINEAGE__JWT_TOKEN: ...
    entrypoint: /bin/bash
    command:
      - "/opt/airflow/ingestion_dependency.sh"
    expose:
      - 8080
    ports:
      - "8090:8080"
    networks:
      - data-pipeline
    volumes:
      - ingestion-volume-dag-airflow:/opt/airflow/dag_generated_configs
      - ingestion-volume-dags:/opt/airflow/dags
      - ingestion-volume-tmp:/tmp
  # Kubeflow Pipelines API Server
  kubeflow-pipelines-api:
    build:
      context: .
      dockerfile: Dockerfile.kubeflow
    container_name: kubeflow-pipelines-api
    ports:
      - "8888:8888"
    environment:
      # Database Configuration for Kubeflow Pipelines Metadata
      DB_HOST: postgres
      DB_PORT: 5432
      DB_NAME: kubeflow_db
      DB_USER: postgres
      DB_PASSWORD: postgres
      # MinIO Configuration for Artifacts
      MINIO_ENDPOINT: http://minio:9000
      MINIO_ACCESS_KEY: admin
      MINIO_SECRET_KEY: password
      MINIO_BUCKET: kubeflow
      # Kubeflow Pipelines Configuration
      KFP_NAMESPACE: kubeflow
      KFP_DEFAULT_EXPERIMENT_NAME: default
    networks:
      - data-pipeline
    depends_on:
      postgres:
        condition: service_started
      minio:
        condition: service_started

  # Kubeflow Pipelines UI (Frontend)
  kubeflow-pipelines-ui:
    image: gcr.io/ml-pipeline/frontend:2.0.0
    container_name: kubeflow-pipelines-ui
    ports:
      - "3000:3000"
    environment:
      API_SERVER_HOST: kubeflow-pipelines-api
      API_SERVER_PORT: 8888
    networks:
      - data-pipeline
    depends_on:
      - kubeflow-pipelines-api
volumes:
  postgres_data:
  postgres_metadata_data:
  minio_data:
  elasticsearch_data:
  ingestion-volume-dag-airflow:
  ingestion-volume-dags:
  ingestion-volume-tmp:

networks:
  data-pipeline:
    name: data-pipeline
    driver: bridge